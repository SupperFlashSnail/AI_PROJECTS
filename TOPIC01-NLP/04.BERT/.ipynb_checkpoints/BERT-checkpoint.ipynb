{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT 模型\n",
    "\n",
    "1. BERT 总体模型：\n",
    "\n",
    "```\n",
    "BERT 总体流程是多个tansformer单元的学习，根据一句话中的前后语境，决定一个词的词向量的值。\n",
    "\n",
    "```\n",
    "\n",
    "<img src='BERT总体流程.jpg' style='zoom:70%'>\n",
    "\n",
    "2. BERT 输入模型：\n",
    "\n",
    "```\n",
    "BERT 的输入需要加入词的位置信息，是否为两句话的判断信息，词语的基本词向量信息，加入位置信息 和 是否为两句话的判断信息的原因是，在transformer的学习过程中需要通过位置 和 是否为 两句话 来学习 部分语法信息，而语法信息有决定了self-attention的单词权重比。\n",
    "\n",
    "```\n",
    "\n",
    "<img src='BERT数据输入.jpg' style='zoom:70%'>\n",
    "\n",
    "3. BERT 的核心单元Transformer模型：\n",
    "\n",
    "```\n",
    "BERT 的核心是Transformer单元，Transformer单元以 上述经过变换后的向量数据作为源输入，最终为每一个词输出 三个矩阵 Q K V，其中Q是指查询矩阵，K是指被查询矩阵，通过 Q·K之间的点乘可以计算出 其他值对当前词语义贡献得分值，最终通过得分值的softmax计算得到每一个词的基本语义V值对当前词语义贡献概率值\n",
    "\n",
    "```\n",
    "\n",
    "<img src='attention求解.png' style='zoom:70%'>\n",
    "\n",
    "\n",
    "```\n",
    "BERT使用transformer的Enconding部分：\n",
    "\n",
    "```\n",
    "\n",
    "<img src='BERT_Transformer单元.png' style='zoom:70%'>\n",
    "\n",
    "```\n",
    "transformer单元：\n",
    "\n",
    "```\n",
    "\n",
    "<img src='Transformer.jpg' style='zoom:70%'>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "4. Transformer多头机制\n",
    "\n",
    "```\n",
    "Transformer多头机制是指 对于每一个词学习 多组 Q K V的向量值，进而使一个词能够表示多重语义，表达更加丰富的语义\n",
    "\n",
    "```\n",
    "<img src='Transformer多头机制.jpg' style='zoom:70%'>\n",
    "\n",
    "```\n",
    "多头机制求解获取多重语义的词向量：\n",
    "\n",
    "```\n",
    "\n",
    "<img src='BERT_QKV1.jpg' style='zoom:70%'>\n",
    "\n",
    "<img src='BERT_QKV2.jpg' style='zoom:70%'>\n",
    "\n",
    "## BERT算法的局限\n",
    "\n",
    "```\n",
    "1. BERT 模型只是通过词语位置信息学习到部分的语法规则\n",
    "2. BERT 模型只能学习出现过的词语，而对于未出现过的词语而无法判别其信息\n",
    "3. 和Word2vec相比，BERT算法模型学习的语义更加精确，能够解决单词多重语义表达的问题，但是所学的词语针对每一种语句都需要学习而无法形成一个可以继承的词向量。\n",
    "\n",
    "```\n",
    "\n",
    "## 针对BERT算法局限性给出的解决方法\n",
    "\n",
    "```\n",
    "在提出解决方案之前需要考虑两个问题：\n",
    "1. BERT模型可以根据self-attention获取单词权重的原因是什么？\n",
    "2. 人类在学习文字时的方式是什么？\n",
    "\n",
    "下面是先说明这两个问题：\n",
    "1. 在语句表达中，有些词的语义会受到前后词的影响，特别是类似于代词的词，甚至包括 中文表达中的语气词。 比如： 干嘛~ 和 干嘛？！所表达的意思完全不同。但是在实际的学习中BERT是粗暴的学习具体一个单词在一个语句中的含义，这个和人类学习时有些差别的。\n",
    "\n",
    "2. 现在思考一下人类学习语言的方法：人类在学习语言时 首先先学习一个基础语义单词 ，然后再 学习 基本的语法，最后再通过不断的读书丰富自己的语言表达和阅读能力\n",
    "\n",
    "学习像苹果 香蕉等基本单词是让人们对单词有个基本的认识，让文字和生活中的模型实体建立一个抽象的连接；\n",
    "\n",
    "学习基本语法 是确定一个基本的语法表达方式，让以后人们基于限定的语法写作，这样一来，后续在阅读过程中可以不用学习就可以将词汇分类。 比如 主语 + 谓语 + 宾语 的规定格式，然后后续文章阅读时人们会对号入座，自然而然的知道 有些词的属性，比如之前学习过一句话： 苹果 是 水果，主 + 系 + 表 的结构，通过这句话的学习 首先可以学习到 “苹果” “水果” 两个词是名词，“是”是一个系动词，当下次 遇到 另一句话： 人 是 动物，对于学习模型而言，没有学习过 人 和 动物 两个词，但是 它 看到 系动词 “是” 的存在，那么网络即可知道 “人” “动物” 是一个名词，而且 在这句话中 动物 对 人这个词的影响较大；\n",
    "\n",
    "阅读文章：阅读文章的目的丰富人们的词汇量。\n",
    "\n",
    "```\n",
    "#### 让算法更加贴近于人类学习方式的解决方法\n",
    "\n",
    "```\n",
    "针对BERT局限性的解决方案是：如下图所示。\n",
    "1. 首先通过机器学习学习一部分基础词的基本语义 和 词属性，构建词袋库\n",
    "2. 然后学习语法规则，即 主语 + 谓语 + 宾语 这个词中主语、谓语、宾语 只能用哪些属性的词\n",
    "3. 文档阅读，首先分词，然后从词袋库中搜索，将 语句中可以遍历到的词的词属性初步确定下来，然后将固定的词放到语法规则中，通过语法规则即可确定其他词的属性，即其他词是名词 还是 动词 即可确定；最后将其他为搜索到的词的此属性补充完整，并将这些词 写入到词袋库中。其中每一步的模型训练都是相互独立的，第二步语法规则的学习 是 使用  BERT模型中的Transformer学习，这个和BERT模型的区别是，此处学习的不是具体单词的self-attention，而是学习 词属性的 self-attention，这样做的好处是，可以解耦。\n",
    "\n",
    "```\n",
    "<img src='针对BERT简单的解决方案.png' style='zoom:70%'>\n",
    "\n",
    "#### 语法规则\n",
    "\n",
    "<img src='语法规则.png' style='zoom:70%'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT pytorch源码解读"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT-pytorch 简介\n",
    "\n",
    "[![LICENSE](https://img.shields.io/github/license/codertimo/BERT-pytorch.svg)](https://github.com/codertimo/BERT-pytorch/blob/master/LICENSE)\n",
    "![GitHub issues](https://img.shields.io/github/issues/codertimo/BERT-pytorch.svg)\n",
    "[![GitHub stars](https://img.shields.io/github/stars/codertimo/BERT-pytorch.svg)](https://github.com/codertimo/BERT-pytorch/stargazers)\n",
    "[![CircleCI](https://circleci.com/gh/codertimo/BERT-pytorch.svg?style=shield)](https://circleci.com/gh/codertimo/BERT-pytorch)\n",
    "[![PyPI](https://img.shields.io/pypi/v/bert-pytorch.svg)](https://pypi.org/project/bert_pytorch/)\n",
    "[![PyPI - Status](https://img.shields.io/pypi/status/bert-pytorch.svg)](https://pypi.org/project/bert_pytorch/)\n",
    "[![Documentation Status](https://readthedocs.org/projects/bert-pytorch/badge/?version=latest)](https://bert-pytorch.readthedocs.io/en/latest/?badge=latest)\n",
    "\n",
    "论文链接: https://arxiv.org/abs/1810.04805\n",
    "\n",
    "\n",
    "\n",
    "## 01.BERT库安装\n",
    "```\n",
    "pip install bert-pytorch \n",
    "```\n",
    "\n",
    "## 02. BERT快速入门\n",
    "\n",
    "**NOTICE : 语料库应该用两个句子组成一行，用tab(\\t)分隔符隔开 如果语料库中一行只有一个句子，则需要修改语料处理代码**\n",
    "\n",
    "#### 0. 准备语料库\n",
    "```\n",
    "Welcome to the \\t the jungle\\n\n",
    "I can stay \\t here all night\\n\n",
    "```\n",
    "\n",
    "or tokenized corpus (tokenization is not in package)\n",
    "```\n",
    "Wel_ _come _to _the \\t _the _jungle\\n\n",
    "_I _can _stay \\t _here _all _night\\n\n",
    "```\n",
    "\n",
    "\n",
    "#### 1. 基于语料库构提词\n",
    "```shell\n",
    "bert -vocab -c data/corpus.small -o data/vocab.small\n",
    "```\n",
    "\n",
    "#### 2. 训练BERT模型\n",
    "```shell\n",
    "bert -c data/corpus.small -v data/vocab.small -o output/bert.model\n",
    "```\n",
    "\n",
    "## 03.语言模型语训练\n",
    "\n",
    "论文中介绍了新的语言模型训练方法：语言mask机制 和 下一句预测 分类两种方法。\n",
    "\n",
    "PS: 1. mask机制就是将一句话中15%内容mask掉，然后让模型预测这些mask的内容 2. 下一句预测方法 是判断两句话根据语义判断第二句话是否和第一句相连。\n",
    "\n",
    "\n",
    "###  mask机制\n",
    "\n",
    "\n",
    "```\n",
    "Input Sequence  : The man went to [MASK] store with [MASK] dog\n",
    "Target Sequence :             the          his\n",
    "```\n",
    "\n",
    "#### 规则:\n",
    "Randomly 15% of input token will be changed into something, based on under sub-rules\n",
    "\n",
    "1. Randomly 80% of tokens, gonna be a `[MASK]` token\n",
    "2. Randomly 10% of tokens, gonna be a `[RANDOM]` token(another word)\n",
    "3. Randomly 10% of tokens, will be remain as same. But need to be predicted.\n",
    "\n",
    "###  句子相关联判断机制\n",
    "\n",
    "```\n",
    "Input : [CLS] the man went to the store [SEP] he bought a gallon of milk [SEP]\n",
    "Label : Is Next\n",
    "\n",
    "Input = [CLS] the man heading to the store [SEP] penguin [MASK] are flight ##less birds [SEP]\n",
    "Label = NotNext\n",
    "```\n",
    "\n",
    "\"两句话是否连续?\"\n",
    "\n",
    "理解两句文本语义以及二者之间的关系判断二者是否连续[标签]\n",
    "\n",
    "#### 规则:\n",
    "\n",
    "构建训练数据 50%语句连续数据， 50%语句不连续数据。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 04. Pytorch BERT源码文件讲解\n",
    "\n",
    "### 源码文件树\n",
    "\n",
    "<img src='BERT_Pytorch.png' style='zoom:80%'>\n",
    "\n",
    "### 源码主要模块\n",
    "\n",
    "```\n",
    "源码主要模块包括：dataset  model  trainer 即 数据集处理模块 BERT模型 训练模型\n",
    "```\n",
    "\n",
    "#### dataset\n",
    "\n",
    "```\n",
    "1. dataset/dataset.py  BERTDataset类 \n",
    "\n",
    "主要功能：实现词汇分割+打标签\n",
    "->将预料库中每一行先按照'\\t'分割符分割为两句话  \n",
    "->分别将每一句话取出，随机mask掉一部分词，并将被mask掉的词作为当前句子的mask预测机制的标签\n",
    "->分为为每一句话的起始位置加上CLS标记 结束位置加上 EOS 标记 \n",
    "-> 对于不满足长度的加上padding 项\n",
    "-> 输出BERT任务tensor数据集:\n",
    "    \"bert_input\": bert_input,   加上mask 首尾标记的语句\n",
    "    \"bert_label\": bert_label,   被mask掉的词，即mask训练机制的label\n",
    "    \"segment_label\": segment_label,  制作的第一句标记为1 第二句话标记为2\n",
    "    \"is_next\": is_next_label         两句话是否连续的label\n",
    "    \n",
    "```\n",
    "\n",
    "```\n",
    "2. dataset/dataset.py  Vocab 类\n",
    "\n",
    "主要功能：实现字与逻辑标号的映射，便于模型训练\n",
    "1. 打开预料映射库，其中包含每一个词的逻辑ID\n",
    "2. 将tensor数据集中的字符换成逻辑ID 值\n",
    "3. 将语句分割标记转换为逻辑ID 包括CLS  EOS  SEP\n",
    "4. 最终将处理数据保存于 指定的--output_pat中\n",
    "\n",
    "```\n",
    "\n",
    "#### model\n",
    "\n",
    "```\n",
    "1. attention模块： transformer 的 多头 机制，其实就是线性神经网络层\n",
    "\n",
    "2. bert.py  实现BERT学习模块的定义\n",
    "  -> 首先使用 BERTEmbedding 将位置信息+基础词向量+段信息 组成BERT模型的输入 \n",
    "  -> 然后经过 多个transformer_block进行BERT模型的学习\n",
    "  \n",
    "  TransformerBlock 模块：\n",
    "  -> 实现多头机制 MultiHeadedAttention\n",
    "  -> 实现位置的前馈网络，激活函数为GELU  PositionwiseFeedForward \n",
    "  -> 特征数据MIN-MAX归一化处理  以及 实现残差网络的block SublayerConnection\n",
    "  -> 特征dropout\n",
    "3. embedding 文件夹,包含几个embeding的辅助函数的py文件：bert.py、position.py、segment.py、token.py，用于将输入的词索引转换为 指定维度可求导的向量\n",
    "\n",
    "4. language_model.py  实现BERT的两种模型预测方式，BERT的原始数据本身没有标签，而是人为的制定预测目标，即 预测mask 或者 预测两句是否相连。因为vocab_size即词的个数较多，固mask预测的输出维度较高。\n",
    "\n",
    "5. transformer.py 实现TransformerBlock模块 实现transformer的Embedding模块\n",
    "\n",
    "6. utils文件夹  包含transformer实现的辅助函数 feed_forward.py-前馈网络  gelu.py-激活函数  layer_norm.py-输出特征标准化处理  sublayer.py 调用layer_norm.py实现特征标准化操作 并 实现残差网络\n",
    "\n",
    "```\n",
    "\n",
    "#### trainer \n",
    "\n",
    "```\n",
    "1. optim_schedule.py  定义损失函数优化器 优化器的权重参数保存 上次batch训练grad清0 定义学习率等操作 \n",
    "  \n",
    "\n",
    "2. pretrain.py   定义BERT模型的训练 测试 保存等接口  其训练的时候 损失值 是next_loss + mask_loss 即目的是 在这两种情况下的损失值都很小，一般mask损失值计算耗时较长，且超参数很大\n",
    "\n",
    "最后训练好的模型会保存于output/bert_trained.model中，使用其他文本预测任务时只需要从训练好的model中取出已经训练完成的语义词向量完成后续任务预测 \n",
    "\n",
    "```\n",
    "\n",
    "## 总结：\n",
    "\n",
    "BERT优点：语义向量 比 word2vec更加精确\n",
    "缺点：对于每一次任务的文本都需要一次训练，且训练超参数很大，耗时较长，而且训练结果并不像word2vec那样，是一个可以继承的固定词向量。产生这个问题主要原因是 词训练的耦合度较高，关于此缺点，上文中已经给出初步解决方案，总结人的学习方式可以方法，这种做法可以解耦，且能生成可以继承使用的词向量，而不需要每次都训练一次，从理论上讲 语义预测更加精确，且有可能实现AI写作文，甚至是代码。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
