{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 词嵌入模型word2vec原理详解\n",
    "\n",
    "### 词嵌入模型概述\n",
    "```\n",
    "word2vec 是通过深度学习的方式将单词映射成具有语义、单词部分属性等信息的N维向量。例如，对于 香蕉 和 苹果两种水果在经过word2vec变换之后，二者之间的向量的欧式距离非常接近。\n",
    "```\n",
    "\n",
    "### 词嵌入模型实现原理\n",
    "```\n",
    "词嵌入式模型的深度学习原理大致如下图所示。词嵌入模型是将一句话中N个词作为源输入，然后以预测第N+1个词为目标，通过不断前向和反向传播，更新前N个词的向量值 和 网络超参数，使预测误差最小，最终获取前N个词的向量值。学习网络一般是 LSTM\n",
    "```\n",
    "<img src='word2vec.png' style='zoom:80%'>\n",
    "\n",
    "```\n",
    "词嵌入式模型的学习目标模型一般有两种：CBOW模型 和 Skip-gram模型\n",
    "\n",
    "1. CBOW模型是一种根据上下文的词语预测当前词语出现概率的模型。[出现的概率越大越好,主要思路相当于最大似然估计]\n",
    "\n",
    "2. Skip-gram模型是CBOW的镜像操作模型，即根据一个词预测其上下文。\n",
    "```\n",
    "\n",
    "<img src='word2vec学习目标模型.png' style='zoom:80%'>\n",
    "\n",
    "####   哈夫曼树\n",
    "\n",
    "```\n",
    "解决问题：快速找到预测目标词\n",
    "\n",
    "由词向量的实现原理可知，词向量是以计算预测词出现的概率为目标，但是词袋中的词语量较大，如何能够快速的找到目标预测词语是词嵌入模型实现需要解决的问题，目标解决的方式是通过哈夫曼树的方式解决。\n",
    "\n",
    "哈夫曼树是一种带权路径长度最短的二叉树，也称为最优二叉树，最优二叉树会将权重较大的值排序越靠前。\n",
    "```\n",
    "\n",
    "<img src='哈夫曼树.png' style='zoom:50%'>\n",
    "\n",
    "#### 哈夫曼树在CBOW中的使用\n",
    "\n",
    "```\n",
    "哈夫曼树主要应用于网络层的输出层，通过多次的softmax函数的二分类，找到最优的目标词。\n",
    "\n",
    "寻找最优词是通过梯度上升方式求解最大似然估计值。\n",
    "\n",
    "具体公式推导可以参考51CTO的课堂讲解：http://www.mamicode.com/info-detail-2489929.html\n",
    "\n",
    "```\n",
    "\n",
    "<img src='哈夫曼树的应用.png' style='zoom:70%'>\n",
    "\n",
    "### 词嵌入模型较概率统计模型的优点\n",
    "\n",
    "```\n",
    "对于 S1 = '我爱你' S2 = '我喜欢你' 两句话，如果语料库中 第一句话出现10次，第二句话出现1000次，那么通过N-Gram模型预测时，P(S1) << P(S2) ,但是对于词嵌入模型预测，P(S1) = P(S2),因为 爱 和 喜欢 两个词的语义和词性都是相似的。\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用word2vec实现影评分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bs4 nltk gensim\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import nltk\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of reviews: 25000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5814_8</td>\n",
       "      <td>1</td>\n",
       "      <td>With all this stuff going down at the moment w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2381_9</td>\n",
       "      <td>1</td>\n",
       "      <td>\"The Classic War of the Worlds\" by Timothy Hin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7759_3</td>\n",
       "      <td>0</td>\n",
       "      <td>The film starts with a manager (Nicholas Bell)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3630_4</td>\n",
       "      <td>0</td>\n",
       "      <td>It must be assumed that those who praised this...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9495_8</td>\n",
       "      <td>1</td>\n",
       "      <td>Superbly trashy and wondrously unpretentious 8...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id  sentiment                                             review\n",
       "0  5814_8          1  With all this stuff going down at the moment w...\n",
       "1  2381_9          1  \"The Classic War of the Worlds\" by Timothy Hin...\n",
       "2  7759_3          0  The film starts with a manager (Nicholas Bell)...\n",
       "3  3630_4          0  It must be assumed that those who praised this...\n",
       "4  9495_8          1  Superbly trashy and wondrously unpretentious 8..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data/labeledTrainData.tsv', sep='\\t', escapechar='\\\\')\n",
    "print('Number of reviews: {}'.format(len(df)))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 对影评数据做预处理，大概有以下环节：\n",
    "\n",
    "1. 去掉html标签\n",
    "1. 移除标点\n",
    "1. 切分成词/token\n",
    "1. 去掉停用词\n",
    "1. 重组为新的句子"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I watched this movie really late last night and usually if it's late then I'm pretty forgiving of movies. Although I tried, I just could not stand this movie at all, it kept getting worse and worse as the movie went on. Although I know it's suppose to be a comedy but I didn't find it very funny. It was also an especially unrealistic, and jaded portrayal of rural life. In case this is what any of you think country life is like, it's definitely not. I do have to agree that some of the guy cast members were cute, but the french guy was really fake. I do have to agree that it tried to have a good lesson in the story, but overall my recommendation is that no one over 8 watch it, it's just too annoying.\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['review'][1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I watched this movie really late last night and usually if it's late then I'm pretty forgiving of movies. Although I tried, I just could not stand this movie at all, it kept getting worse and worse as the movie went on. Although I know it's suppose to be a comedy but I didn't find it very funny. It was also an especially unrealistic, and jaded portrayal of rural life. In case this is what any of you think country life is like, it's definitely not. I do have to agree that some of the guy cast members were cute, but the french guy was really fake. I do have to agree that it tried to have a good lesson in the story, but overall my recommendation is that no one over 8 watch it, it's just too annoying.\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#去掉HTML标签的数据\n",
    "example = BeautifulSoup(df['review'][1000], 'html.parser').get_text()\n",
    "example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I watched this movie really late last night and usually if it s late then I m pretty forgiving of movies  Although I tried  I just could not stand this movie at all  it kept getting worse and worse as the movie went on  Although I know it s suppose to be a comedy but I didn t find it very funny  It was also an especially unrealistic  and jaded portrayal of rural life  In case this is what any of you think country life is like  it s definitely not  I do have to agree that some of the guy cast members were cute  but the french guy was really fake  I do have to agree that it tried to have a good lesson in the story  but overall my recommendation is that no one over   watch it  it s just too annoying '"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#去掉标点符号\n",
    "example_letters = re.sub(r'[^a-zA-Z]', ' ', example)\n",
    "example_letters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'watched',\n",
       " 'this',\n",
       " 'movie',\n",
       " 'really',\n",
       " 'late',\n",
       " 'last',\n",
       " 'night',\n",
       " 'and',\n",
       " 'usually',\n",
       " 'if',\n",
       " 'it',\n",
       " 's',\n",
       " 'late',\n",
       " 'then',\n",
       " 'i',\n",
       " 'm',\n",
       " 'pretty',\n",
       " 'forgiving',\n",
       " 'of',\n",
       " 'movies',\n",
       " 'although',\n",
       " 'i',\n",
       " 'tried',\n",
       " 'i',\n",
       " 'just',\n",
       " 'could',\n",
       " 'not',\n",
       " 'stand',\n",
       " 'this',\n",
       " 'movie',\n",
       " 'at',\n",
       " 'all',\n",
       " 'it',\n",
       " 'kept',\n",
       " 'getting',\n",
       " 'worse',\n",
       " 'and',\n",
       " 'worse',\n",
       " 'as',\n",
       " 'the',\n",
       " 'movie',\n",
       " 'went',\n",
       " 'on',\n",
       " 'although',\n",
       " 'i',\n",
       " 'know',\n",
       " 'it',\n",
       " 's',\n",
       " 'suppose',\n",
       " 'to',\n",
       " 'be',\n",
       " 'a',\n",
       " 'comedy',\n",
       " 'but',\n",
       " 'i',\n",
       " 'didn',\n",
       " 't',\n",
       " 'find',\n",
       " 'it',\n",
       " 'very',\n",
       " 'funny',\n",
       " 'it',\n",
       " 'was',\n",
       " 'also',\n",
       " 'an',\n",
       " 'especially',\n",
       " 'unrealistic',\n",
       " 'and',\n",
       " 'jaded',\n",
       " 'portrayal',\n",
       " 'of',\n",
       " 'rural',\n",
       " 'life',\n",
       " 'in',\n",
       " 'case',\n",
       " 'this',\n",
       " 'is',\n",
       " 'what',\n",
       " 'any',\n",
       " 'of',\n",
       " 'you',\n",
       " 'think',\n",
       " 'country',\n",
       " 'life',\n",
       " 'is',\n",
       " 'like',\n",
       " 'it',\n",
       " 's',\n",
       " 'definitely',\n",
       " 'not',\n",
       " 'i',\n",
       " 'do',\n",
       " 'have',\n",
       " 'to',\n",
       " 'agree',\n",
       " 'that',\n",
       " 'some',\n",
       " 'of',\n",
       " 'the',\n",
       " 'guy',\n",
       " 'cast',\n",
       " 'members',\n",
       " 'were',\n",
       " 'cute',\n",
       " 'but',\n",
       " 'the',\n",
       " 'french',\n",
       " 'guy',\n",
       " 'was',\n",
       " 'really',\n",
       " 'fake',\n",
       " 'i',\n",
       " 'do',\n",
       " 'have',\n",
       " 'to',\n",
       " 'agree',\n",
       " 'that',\n",
       " 'it',\n",
       " 'tried',\n",
       " 'to',\n",
       " 'have',\n",
       " 'a',\n",
       " 'good',\n",
       " 'lesson',\n",
       " 'in',\n",
       " 'the',\n",
       " 'story',\n",
       " 'but',\n",
       " 'overall',\n",
       " 'my',\n",
       " 'recommendation',\n",
       " 'is',\n",
       " 'that',\n",
       " 'no',\n",
       " 'one',\n",
       " 'over',\n",
       " 'watch',\n",
       " 'it',\n",
       " 'it',\n",
       " 's',\n",
       " 'just',\n",
       " 'too',\n",
       " 'annoying']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = example_letters.lower().split()\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['watched',\n",
       " 'movie',\n",
       " 'late',\n",
       " 'night',\n",
       " 'late',\n",
       " 'pretty',\n",
       " 'forgiving',\n",
       " 'movies',\n",
       " 'stand',\n",
       " 'movie',\n",
       " 'worse',\n",
       " 'worse',\n",
       " 'movie',\n",
       " 'suppose',\n",
       " 'comedy',\n",
       " 'didn',\n",
       " 'funny',\n",
       " 'unrealistic',\n",
       " 'jaded',\n",
       " 'portrayal',\n",
       " 'rural',\n",
       " 'life',\n",
       " 'country',\n",
       " 'life',\n",
       " 'agree',\n",
       " 'guy',\n",
       " 'cast',\n",
       " 'cute',\n",
       " 'french',\n",
       " 'guy',\n",
       " 'fake',\n",
       " 'agree',\n",
       " 'lesson',\n",
       " 'story',\n",
       " 'recommendation',\n",
       " 'watch',\n",
       " 'annoying']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#去停用词\n",
    "stopwords = {}.fromkeys([ line.rstrip() for line in open('stopwords.txt')])\n",
    "words_nostop = [w for w in words if w not in stopwords]\n",
    "words_nostop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_stopwords = set(stopwords)\n",
    "\n",
    "def clean_text(text):\n",
    "    text = BeautifulSoup(text, 'html.parser').get_text()\n",
    "    text = re.sub(r'[^a-zA-Z]', ' ', text)\n",
    "    words = text.lower().split()\n",
    "    words = [w for w in words if w not in eng_stopwords]\n",
    "    return ' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I watched this movie really late last night and usually if it's late then I'm pretty forgiving of movies. Although I tried, I just could not stand this movie at all, it kept getting worse and worse as the movie went on. Although I know it's suppose to be a comedy but I didn't find it very funny. It was also an especially unrealistic, and jaded portrayal of rural life. In case this is what any of you think country life is like, it's definitely not. I do have to agree that some of the guy cast members were cute, but the french guy was really fake. I do have to agree that it tried to have a good lesson in the story, but overall my recommendation is that no one over 8 watch it, it's just too annoying.\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['review'][1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'watched movie late night late pretty forgiving movies stand movie worse worse movie suppose comedy didn funny unrealistic jaded portrayal rural life country life agree guy cast cute french guy fake agree lesson story recommendation watch annoying'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_text(df['review'][1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 清洗数据添加到dataframe里"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>review</th>\n",
       "      <th>clean_review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5814_8</td>\n",
       "      <td>1</td>\n",
       "      <td>With all this stuff going down at the moment w...</td>\n",
       "      <td>stuff moment mj ve started listening music wat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2381_9</td>\n",
       "      <td>1</td>\n",
       "      <td>\"The Classic War of the Worlds\" by Timothy Hin...</td>\n",
       "      <td>classic war worlds timothy hines entertaining ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7759_3</td>\n",
       "      <td>0</td>\n",
       "      <td>The film starts with a manager (Nicholas Bell)...</td>\n",
       "      <td>film starts manager nicholas bell investors ro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3630_4</td>\n",
       "      <td>0</td>\n",
       "      <td>It must be assumed that those who praised this...</td>\n",
       "      <td>assumed praised film filmed opera didn read do...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9495_8</td>\n",
       "      <td>1</td>\n",
       "      <td>Superbly trashy and wondrously unpretentious 8...</td>\n",
       "      <td>superbly trashy wondrously unpretentious explo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id  sentiment                                             review  \\\n",
       "0  5814_8          1  With all this stuff going down at the moment w...   \n",
       "1  2381_9          1  \"The Classic War of the Worlds\" by Timothy Hin...   \n",
       "2  7759_3          0  The film starts with a manager (Nicholas Bell)...   \n",
       "3  3630_4          0  It must be assumed that those who praised this...   \n",
       "4  9495_8          1  Superbly trashy and wondrously unpretentious 8...   \n",
       "\n",
       "                                        clean_review  \n",
       "0  stuff moment mj ve started listening music wat...  \n",
       "1  classic war worlds timothy hines entertaining ...  \n",
       "2  film starts manager nicholas bell investors ro...  \n",
       "3  assumed praised film filmed opera didn read do...  \n",
       "4  superbly trashy wondrously unpretentious explo...  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['clean_review'] = df.review.apply(clean_text)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  抽取bag of words特征(用sklearn的CountVectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 5000)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(max_features = 5000) \n",
    "train_data_features = vectorizer.fit_transform(df.clean_review).toarray()\n",
    "train_data_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_data_features,df.sentiment,test_size = 0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    \"\"\"\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=0)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j],\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训练分类器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PC\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall metric in the testing dataset:  0.8531810766721044\n",
      "accuracy metric in the testing dataset:  0.8454\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "LR_model = LogisticRegression()\n",
    "LR_model = LR_model.fit(X_train, y_train)\n",
    "y_pred = LR_model.predict(X_test)\n",
    "cnf_matrix = confusion_matrix(y_test,y_pred)\n",
    "\n",
    "print(\"Recall metric in the testing dataset: \", cnf_matrix[1,1]/(cnf_matrix[1,0]+cnf_matrix[1,1]))\n",
    "\n",
    "print(\"accuracy metric in the testing dataset: \", (cnf_matrix[1,1]+cnf_matrix[0,0])/(cnf_matrix[0,0]+cnf_matrix[1,1]+cnf_matrix[1,0]+cnf_matrix[0,1]))\n",
    "\n",
    "# Plot non-normalized confusion matrix\n",
    "class_names = [0,1]\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix\n",
    "                      , classes=class_names\n",
    "                      , title='Confusion matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of reviews: 50000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9999_0</td>\n",
       "      <td>Watching Time Chasers, it obvious that it was ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>45057_0</td>\n",
       "      <td>I saw this film about 20 years ago and remembe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15561_0</td>\n",
       "      <td>Minor Spoilers&lt;br /&gt;&lt;br /&gt;In New York, Joan Ba...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7161_0</td>\n",
       "      <td>I went to see this film with a great deal of e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>43971_0</td>\n",
       "      <td>Yes, I agree with everyone on this site this m...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                             review\n",
       "0   9999_0  Watching Time Chasers, it obvious that it was ...\n",
       "1  45057_0  I saw this film about 20 years ago and remembe...\n",
       "2  15561_0  Minor Spoilers<br /><br />In New York, Joan Ba...\n",
       "3   7161_0  I went to see this film with a great deal of e...\n",
       "4  43971_0  Yes, I agree with everyone on this site this m..."
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data/unlabeledTrainData.tsv', sep='\\t', escapechar='\\\\')\n",
    "print('Number of reviews: {}'.format(len(df)))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>review</th>\n",
       "      <th>clean_review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9999_0</td>\n",
       "      <td>Watching Time Chasers, it obvious that it was ...</td>\n",
       "      <td>watching time chasers obvious bunch friends si...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>45057_0</td>\n",
       "      <td>I saw this film about 20 years ago and remembe...</td>\n",
       "      <td>film ago remember nasty based true incident br...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15561_0</td>\n",
       "      <td>Minor Spoilers&lt;br /&gt;&lt;br /&gt;In New York, Joan Ba...</td>\n",
       "      <td>minor spoilersin york joan barnard elvire audr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7161_0</td>\n",
       "      <td>I went to see this film with a great deal of e...</td>\n",
       "      <td>film deal excitement school director friend mi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>43971_0</td>\n",
       "      <td>Yes, I agree with everyone on this site this m...</td>\n",
       "      <td>agree site movie bad call movie insult movies ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                             review  \\\n",
       "0   9999_0  Watching Time Chasers, it obvious that it was ...   \n",
       "1  45057_0  I saw this film about 20 years ago and remembe...   \n",
       "2  15561_0  Minor Spoilers<br /><br />In New York, Joan Ba...   \n",
       "3   7161_0  I went to see this film with a great deal of e...   \n",
       "4  43971_0  Yes, I agree with everyone on this site this m...   \n",
       "\n",
       "                                        clean_review  \n",
       "0  watching time chasers obvious bunch friends si...  \n",
       "1  film ago remember nasty based true incident br...  \n",
       "2  minor spoilersin york joan barnard elvire audr...  \n",
       "3  film deal excitement school director friend mi...  \n",
       "4  agree site movie bad call movie insult movies ...  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['clean_review'] = df.review.apply(clean_text)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000,)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_part = df['clean_review']\n",
    "review_part.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "\n",
    "\n",
    "def split_sentences(review):\n",
    "    raw_sentences = tokenizer.tokenize(review.strip())\n",
    "    sentences = [clean_text(s) for s in raw_sentences if s]\n",
    "    return sentences\n",
    "sentences = sum(review_part.apply(split_sentences), [])\n",
    "print('{} reviews -> {} sentences'.format(len(review_part), len(sentences)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_list = []\n",
    "for line in sentences:\n",
    "    sentences_list.append(nltk.word_tokenize(line))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-  sentences：可以是一个list\n",
    "-  sg： 用于设置训练算法，默认为0，对应CBOW算法；sg=1则采用skip-gram算法。\n",
    "-  size：是指特征向量的维度，默认为100。大的size需要更多的训练数据,但是效果会更好. 推荐值为几十到几百。\n",
    "-  window：表示当前词与预测词在一个句子中的最大距离是多少\n",
    "-  alpha: 是学习速率\n",
    "-  seed：用于随机数发生器。与初始化词向量有关。\n",
    "-  min_count: 可以对字典做截断. 词频少于min_count次数的单词会被丢弃掉, 默认值为5\n",
    "-  max_vocab_size: 设置词向量构建期间的RAM限制。如果所有独立单词个数超过这个，则就消除掉其中最不频繁的一个。每一千万个单词需要大约1GB的RAM。设置成None则没有限制。\n",
    "\n",
    "-  workers参数控制训练的并行数。\n",
    "-  hs: 如果为1则会采用hierarchica·softmax技巧。如果设置为0（defau·t），则negative sampling会被使用。\n",
    "-  negative: 如果>0,则会采用negativesamp·ing，用于设置多少个noise words\n",
    "-  iter： 迭代次数，默认为5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设定词向量训练的参数\n",
    "num_features = 300    # Word vector dimensionality\n",
    "min_word_count = 40   # Minimum word count\n",
    "num_workers = 4       # Number of threads to run in parallel\n",
    "context = 10          # Context window size\n",
    "model_name = '{}features_{}minwords_{}context.model'.format(num_features, min_word_count, context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.word2vec import Word2Vec\n",
    "model = Word2Vec(sentences_list, workers=num_workers, \\\n",
    "            size=num_features, min_count = min_word_count, \\\n",
    "            window = context)\n",
    "\n",
    "# If you don't plan to train the model any further, calling \n",
    "# init_sims will make the model much more memory-efficient.\n",
    "model.init_sims(replace=True)\n",
    "\n",
    "# It can be helpful to create a meaningful model name and \n",
    "# save the model for later use. You can load it later using Word2Vec.load()\n",
    "model.save(os.path.join('..', 'models', model_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.doesnt_match(['man','woman','child','kitchen']))\n",
    "#print(model.doesnt_match('france england germany berlin'.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.most_similar(\"boy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.most_similar(\"bad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/labeledTrainData.tsv', sep='\\t', escapechar='\\\\')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "eng_stopwords = set(stopwords.words('english'))\n",
    "\n",
    "def clean_text(text, remove_stopwords=False):\n",
    "    text = BeautifulSoup(text, 'html.parser').get_text()\n",
    "    text = re.sub(r'[^a-zA-Z]', ' ', text)\n",
    "    words = text.lower().split()\n",
    "    if remove_stopwords:\n",
    "        words = [w for w in words if w not in eng_stopwords]\n",
    "    return words\n",
    "\n",
    "def to_review_vector(review):\n",
    "    global word_vec\n",
    "    \n",
    "    review = clean_text(review, remove_stopwords=True)\n",
    "    #print (review)\n",
    "    #words = nltk.word_tokenize(review)\n",
    "    word_vec = np.zeros((1,300))\n",
    "    for word in review:\n",
    "        #word_vec = np.zeros((1,300))\n",
    "        if word in model:\n",
    "            word_vec += np.array([model[word]])\n",
    "    #print (word_vec.mean(axis = 0))\n",
    "    return pd.Series(word_vec.mean(axis = 0))\n",
    "\n",
    "train_data_features = df.review.apply(to_review_vector)\n",
    "train_data_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_data_features,df.sentiment,test_size = 0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR_model = LogisticRegression()\n",
    "LR_model = LR_model.fit(X_train, y_train)\n",
    "y_pred = LR_model.predict(X_test)\n",
    "cnf_matrix = confusion_matrix(y_test,y_pred)\n",
    "\n",
    "print(\"Recall metric in the testing dataset: \", cnf_matrix[1,1]/(cnf_matrix[1,0]+cnf_matrix[1,1]))\n",
    "\n",
    "print(\"accuracy metric in the testing dataset: \", (cnf_matrix[1,1]+cnf_matrix[0,0])/(cnf_matrix[0,0]+cnf_matrix[1,1]+cnf_matrix[1,0]+cnf_matrix[0,1]))\n",
    "\n",
    "# Plot non-normalized confusion matrix\n",
    "class_names = [0,1]\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix\n",
    "                      , classes=class_names\n",
    "                      , title='Confusion matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorflow实现wor2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import math\n",
    "import collections\n",
    "import pickle as pkl\n",
    "from pprint import pprint\n",
    "#from pymongo import MongoClient\n",
    "import re\n",
    "import jieba\n",
    "import os.path as path\n",
    "import os\n",
    "\n",
    "class word2vec():\n",
    "    def __init__(self,\n",
    "                 vocab_list=None,\n",
    "                 embedding_size=200,\n",
    "                 win_len=3, # 单边窗口长\n",
    "                 num_sampled=1000,\n",
    "                 learning_rate=1.0,\n",
    "                 logdir='/tmp/simple_word2vec',\n",
    "                 model_path= None\n",
    "                 ):\n",
    "\n",
    "        # 获得模型的基本参数\n",
    "        self.batch_size     = None # 一批中数据个数, 目前是根据情况来的\n",
    "        if model_path!=None:\n",
    "            self.load_model(model_path)\n",
    "        else:\n",
    "            # model parameters\n",
    "            assert type(vocab_list)==list\n",
    "            self.vocab_list     = vocab_list\n",
    "            self.vocab_size     = vocab_list.__len__()\n",
    "            self.embedding_size = embedding_size\n",
    "            self.win_len        = win_len\n",
    "            self.num_sampled    = num_sampled\n",
    "            self.learning_rate  = learning_rate\n",
    "            self.logdir         = logdir\n",
    "\n",
    "            self.word2id = {}   # word => id 的映射\n",
    "            for i in range(self.vocab_size):\n",
    "                self.word2id[self.vocab_list[i]] = i\n",
    "\n",
    "            # train times\n",
    "            self.train_words_num = 0 # 训练的单词对数\n",
    "            self.train_sents_num = 0 # 训练的句子数\n",
    "            self.train_times_num = 0 # 训练的次数（一次可以有多个句子）\n",
    "\n",
    "            # train loss records\n",
    "            self.train_loss_records = collections.deque(maxlen=10) # 保存最近10次的误差\n",
    "            self.train_loss_k10 = 0\n",
    "\n",
    "        self.build_graph()\n",
    "        self.init_op()\n",
    "        if model_path!=None:\n",
    "            tf_model_path = os.path.join(model_path,'tf_vars')\n",
    "            self.saver.restore(self.sess,tf_model_path)\n",
    "\n",
    "    def init_op(self):\n",
    "        self.sess = tf.Session(graph=self.graph)\n",
    "        self.sess.run(self.init)\n",
    "        self.summary_writer = tf.train.SummaryWriter(self.logdir, self.sess.graph)\n",
    "\n",
    "    def build_graph(self):\n",
    "        self.graph = tf.Graph()\n",
    "        with self.graph.as_default():\n",
    "            self.train_inputs = tf.placeholder(tf.int32, shape=[self.batch_size])\n",
    "            self.train_labels = tf.placeholder(tf.int32, shape=[self.batch_size, 1])\n",
    "            self.embedding_dict = tf.Variable(\n",
    "                tf.random_uniform([self.vocab_size,self.embedding_size],-1.0,1.0)\n",
    "            )\n",
    "            self.nce_weight = tf.Variable(tf.truncated_normal([self.vocab_size, self.embedding_size],\n",
    "                                                              stddev=1.0/math.sqrt(self.embedding_size)))\n",
    "            self.nce_biases = tf.Variable(tf.zeros([self.vocab_size]))\n",
    "\n",
    "            # 将输入序列向量化\n",
    "            embed = tf.nn.embedding_lookup(self.embedding_dict, self.train_inputs) # batch_size\n",
    "\n",
    "            # 得到NCE损失\n",
    "            self.loss = tf.reduce_mean(\n",
    "                tf.nn.nce_loss(\n",
    "                    weights = self.nce_weight,\n",
    "                    biases = self.nce_biases,\n",
    "                    labels = self.train_labels,\n",
    "                    inputs = embed,\n",
    "                    num_sampled = self.num_sampled,\n",
    "                    num_classes = self.vocab_size\n",
    "                )\n",
    "            )\n",
    "\n",
    "            # tensorboard 相关\n",
    "            tf.summary.scalar('loss',self.loss)  # 让tensorflow记录参数\n",
    "\n",
    "            # 根据 nce loss 来更新梯度和embedding\n",
    "            self.train_op = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(self.loss)  # 训练操作\n",
    "\n",
    "            # 计算与指定若干单词的相似度\n",
    "            self.test_word_id = tf.placeholder(tf.int32,shape=[None])\n",
    "            vec_l2_model = tf.sqrt(  # 求各词向量的L2模\n",
    "                tf.reduce_sum(tf.square(self.embedding_dict),1,keep_dims=True)\n",
    "            )\n",
    "\n",
    "            avg_l2_model = tf.reduce_mean(vec_l2_model)\n",
    "            tf.summary.scalar('avg_vec_model',avg_l2_model)\n",
    "\n",
    "            self.normed_embedding = self.embedding_dict / vec_l2_model\n",
    "            # self.embedding_dict = norm_vec # 对embedding向量正则化\n",
    "            test_embed = tf.nn.embedding_lookup(self.normed_embedding, self.test_word_id)\n",
    "            self.similarity = tf.matmul(test_embed, self.normed_embedding, transpose_b=True)\n",
    "\n",
    "            # 变量初始化\n",
    "            self.init = tf.global_variables_initializer()\n",
    "\n",
    "            self.merged_summary_op = tf.merge_all_summaries()\n",
    "\n",
    "            self.saver = tf.train.Saver()\n",
    "\n",
    "    def train_by_sentence(self, input_sentence=[]):\n",
    "        #  input_sentence: [sub_sent1, sub_sent2, ...]\n",
    "        # 每个sub_sent是一个单词序列，例如['这次','大选','让']\n",
    "        sent_num = input_sentence.__len__()\n",
    "        batch_inputs = []\n",
    "        batch_labels = []\n",
    "        for sent in input_sentence:\n",
    "            for i in range(sent.__len__()):\n",
    "                start = max(0,i-self.win_len)\n",
    "                end = min(sent.__len__(),i+self.win_len+1)\n",
    "                for index in range(start,end):\n",
    "                    if index == i:\n",
    "                        continue\n",
    "                    else:\n",
    "                        input_id = self.word2id.get(sent[i])\n",
    "                        label_id = self.word2id.get(sent[index])\n",
    "                        if not (input_id and label_id):\n",
    "                            continue\n",
    "                        batch_inputs.append(input_id)\n",
    "                        batch_labels.append(label_id)\n",
    "        if len(batch_inputs)==0:\n",
    "            return\n",
    "        batch_inputs = np.array(batch_inputs,dtype=np.int32)\n",
    "        batch_labels = np.array(batch_labels,dtype=np.int32)\n",
    "        batch_labels = np.reshape(batch_labels,[batch_labels.__len__(),1])\n",
    "\n",
    "        feed_dict = {\n",
    "            self.train_inputs: batch_inputs,\n",
    "            self.train_labels: batch_labels\n",
    "        }\n",
    "        _, loss_val, summary_str = self.sess.run([self.train_op,self.loss,self.merged_summary_op], feed_dict=feed_dict)\n",
    "\n",
    "        # train loss\n",
    "        self.train_loss_records.append(loss_val)\n",
    "        # self.train_loss_k10 = sum(self.train_loss_records)/self.train_loss_records.__len__()\n",
    "        self.train_loss_k10 = np.mean(self.train_loss_records)\n",
    "        if self.train_sents_num % 1000 == 0 :\n",
    "            self.summary_writer.add_summary(summary_str,self.train_sents_num)\n",
    "            print(\"{a} sentences dealed, loss: {b}\"\n",
    "                  .format(a=self.train_sents_num,b=self.train_loss_k10))\n",
    "\n",
    "        # train times\n",
    "        self.train_words_num += batch_inputs.__len__()\n",
    "        self.train_sents_num += input_sentence.__len__()\n",
    "        self.train_times_num += 1\n",
    "\n",
    "    def cal_similarity(self,test_word_id_list,top_k=10):\n",
    "        sim_matrix = self.sess.run(self.similarity, feed_dict={self.test_word_id:test_word_id_list})\n",
    "        sim_mean = np.mean(sim_matrix)\n",
    "        sim_var = np.mean(np.square(sim_matrix-sim_mean))\n",
    "        test_words = []\n",
    "        near_words = []\n",
    "        for i in range(test_word_id_list.__len__()):\n",
    "            test_words.append(self.vocab_list[test_word_id_list[i]])\n",
    "            nearst_id = (-sim_matrix[i,:]).argsort()[1:top_k+1]\n",
    "            nearst_word = [self.vocab_list[x] for x in nearst_id]\n",
    "            near_words.append(nearst_word)\n",
    "        return test_words,near_words,sim_mean,sim_var\n",
    "\n",
    "    def save_model(self, save_path):\n",
    "\n",
    "        if os.path.isfile(save_path):\n",
    "            raise RuntimeError('the save path should be a dir')\n",
    "        if not os.path.exists(save_path):\n",
    "            os.mkdir(save_path)\n",
    "\n",
    "        # 记录模型各参数\n",
    "        model = {}\n",
    "        var_names = ['vocab_size',      # int       model parameters\n",
    "                     'vocab_list',      # list\n",
    "                     'learning_rate',   # int\n",
    "                     'word2id',         # dict\n",
    "                     'embedding_size',  # int\n",
    "                     'logdir',          # str\n",
    "                     'win_len',         # int\n",
    "                     'num_sampled',     # int\n",
    "                     'train_words_num', # int       train info\n",
    "                     'train_sents_num', # int\n",
    "                     'train_times_num', # int\n",
    "                     'train_loss_records',  # int   train loss\n",
    "                     'train_loss_k10',  # int\n",
    "                     ]\n",
    "        for var in var_names:\n",
    "            model[var] = eval('self.'+var)\n",
    "\n",
    "        param_path = os.path.join(save_path,'params.pkl')\n",
    "        if os.path.exists(param_path):\n",
    "            os.remove(param_path)\n",
    "        with open(param_path,'wb') as f:\n",
    "            pkl.dump(model,f)\n",
    "\n",
    "        # 记录tf模型\n",
    "        tf_path = os.path.join(save_path,'tf_vars')\n",
    "        if os.path.exists(tf_path):\n",
    "            os.remove(tf_path)\n",
    "        self.saver.save(self.sess,tf_path)\n",
    "\n",
    "    def load_model(self, model_path):\n",
    "        if not os.path.exists(model_path):\n",
    "            raise RuntimeError('file not exists')\n",
    "        param_path = os.path.join(model_path,'params.pkl')\n",
    "        with open(param_path,'rb') as f:\n",
    "            model = pkl.load(f)\n",
    "            self.vocab_list = model['vocab_list']\n",
    "            self.vocab_size = model['vocab_size']\n",
    "            self.logdir = model['logdir']\n",
    "            self.word2id = model['word2id']\n",
    "            self.embedding_size = model['embedding_size']\n",
    "            self.learning_rate = model['learning_rate']\n",
    "            self.win_len = model['win_len']\n",
    "            self.num_sampled = model['num_sampled']\n",
    "            self.train_words_num = model['train_words_num']\n",
    "            self.train_sents_num = model['train_sents_num']\n",
    "            self.train_times_num = model['train_times_num']\n",
    "            self.train_loss_records = model['train_loss_records']\n",
    "            self.train_loss_k10 = model['train_loss_k10']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "停用词读取完毕，共1893个单词\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dumping model to file cache C:\\Users\\PC\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.658 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "文本中总共有19780个单词,不重复单词数5568,选取前30000个单词进入词典\n",
      "WARNING:tensorflow:From C:\\Users\\PC\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow' has no attribute 'scalar_summary'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-376870790dd9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     42\u001b[0m                    \u001b[0mlearning_rate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m                    \u001b[0mnum_sampled\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m         \u001b[1;31m# 负采样个数\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m                    logdir='/tmp/280')       # tensorboard记录地址\n\u001b[0m\u001b[0;32m     45\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-7c93b847e0ff>\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, vocab_list, embedding_size, win_len, num_sampled, learning_rate, logdir, model_path)\u001b[0m\n\u001b[0;32m     50\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_loss_k10\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 52\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     53\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minit_op\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mmodel_path\u001b[0m\u001b[1;33m!=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-7c93b847e0ff>\u001b[0m in \u001b[0;36mbuild_graph\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     89\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     90\u001b[0m             \u001b[1;31m# tensorboard 相关\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 91\u001b[1;33m             \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscalar_summary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'loss'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# 让tensorflow记录参数\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     92\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m             \u001b[1;31m# 根据 nce loss 来更新梯度和embedding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\deprecation_wrapper.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m    104\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'_dw_'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    105\u001b[0m       \u001b[1;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Accessing local variables before they are created.'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 106\u001b[1;33m     \u001b[0mattr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dw_wrapped_module\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    107\u001b[0m     if (self._dw_warning_count < _PER_MODULE_WARNING_LIMIT and\n\u001b[0;32m    108\u001b[0m         name not in self._dw_deprecated_printed):\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'tensorflow' has no attribute 'scalar_summary'"
     ]
    }
   ],
   "source": [
    "if __name__=='__main__':\n",
    "\n",
    "    # step 1 读取停用词\n",
    "    stop_words = []\n",
    "    with open('stop_words.txt',encoding= 'utf-8') as f:\n",
    "        line = f.readline()\n",
    "        while line:\n",
    "            stop_words.append(line[:-1])\n",
    "            line = f.readline()\n",
    "    stop_words = set(stop_words)\n",
    "    print('停用词读取完毕，共{n}个单词'.format(n=len(stop_words)))\n",
    "\n",
    "    # step2 读取文本，预处理，分词，得到词典\n",
    "    raw_word_list = []\n",
    "    sentence_list = []\n",
    "    with open('2800.txt',encoding='gbk') as f:\n",
    "        line = f.readline()\n",
    "        while line:\n",
    "            while '\\n' in line:\n",
    "                line = line.replace('\\n','')\n",
    "            while ' ' in line:\n",
    "                line = line.replace(' ','')\n",
    "            if len(line)>0: # 如果句子非空\n",
    "                raw_words = list(jieba.cut(line,cut_all=False))\n",
    "                dealed_words = []\n",
    "                for word in raw_words:\n",
    "                    if word not in stop_words and word not in ['qingkan520','www','com','http']:\n",
    "                        raw_word_list.append(word)\n",
    "                        dealed_words.append(word)\n",
    "                sentence_list.append(dealed_words)\n",
    "            line = f.readline()\n",
    "    word_count = collections.Counter(raw_word_list)\n",
    "    print('文本中总共有{n1}个单词,不重复单词数{n2},选取前30000个单词进入词典'\n",
    "          .format(n1=len(raw_word_list),n2=len(word_count)))\n",
    "    word_count = word_count.most_common(30000)\n",
    "    word_list = [x[0] for x in word_count]\n",
    "\n",
    "    # 创建模型，训练\n",
    "    w2v = word2vec(vocab_list=word_list,    # 词典集\n",
    "                   embedding_size=200,\n",
    "                   win_len=2,\n",
    "                   learning_rate=1,\n",
    "                   num_sampled=100,         # 负采样个数\n",
    "                   logdir='/tmp/280')       # tensorboard记录地址\n",
    "    \n",
    "\n",
    "    num_steps = 10000\n",
    "    for i in range(num_steps):\n",
    "        #print (i%len(sentence_list))\n",
    "        sent = sentence_list[i%len(sentence_list)]\n",
    "        w2v.train_by_sentence([sent])\n",
    "    w2v.save_model('model')\n",
    "    \n",
    "    w2v.load_model('model') \n",
    "    test_word = ['天地','级别']\n",
    "    test_id = [word_list.index(x) for x in test_word]\n",
    "    test_words,near_words,sim_mean,sim_var = w2v.cal_similarity(test_id)\n",
    "    print (test_words,near_words,sim_mean,sim_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
