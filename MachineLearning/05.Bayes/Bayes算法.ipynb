{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayes算法概述\n",
    "```\n",
    "贝叶斯定理是关于随机事件A和B的条件概率定理，即在条件B发生时A发生的概率\n",
    "```\n",
    "### bayes公式\n",
    "<img src=\"Bayes公式.png\" style='zoom:100%' >\n",
    "\n",
    "### bayes公式物理含义\n",
    "```\n",
    "邮件中出现词A，通过此可视现象推断当前邮件为垃圾邮件的概率：\n",
    "首先加入垃圾邮件和普通邮件出现概率相同时可以计算词A出现的概率：PA+PB。如果两种邮件出现概率不同，则需乘以对应概率即可。\n",
    "然后计算词A在垃圾邮件中出现的概率PA\n",
    "最后使用PA/(PA+PB) 即可得到词A出现时，当前邮件为垃圾邮件的概率\n",
    "```\n",
    "<img src=\"bayes公式图形.png\" style='zoom:50%' >\n",
    "\n",
    "### 朴素贝叶斯\n",
    "```\n",
    "朴素贝叶斯要求条件相对独立\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.基于贝叶斯算法的单词纠错实例\n",
    "```\n",
    "总体流程：如下图所示。\n",
    "单词的权重计算：本文以计算单词在预料库中出现的次数为单词权重。单词权重计算还可以与神经网络结合，即通过神经网络学习当前语境信息，然后根据语境信息确定当前词汇权重值。\n",
    "\n",
    "本文基于预料库计算时，预料库需要根据个人近期阅读和书写内容实时更新预料库内容，如果预料库信息不全时则会导致预测准确率降低。\n",
    "```\n",
    "<img src=\"基于贝叶斯单词纠错流程.png\" style='zoom:50%' >"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re,collections\n",
    "\n",
    "# 语料库预处理\n",
    "def words(text):\n",
    "    return re.findall('[a-z]+', text.lower())\n",
    "\n",
    "# 语料词汇数量统计\n",
    "def word_record_get(features):\n",
    "    model = collections.defaultdict(lambda: 1)\n",
    "    for f in features:\n",
    "        model[f] += 1\n",
    "    return model\n",
    "\n",
    "NWORDS = word_record_get(words(open('big.txt').read()))       #基于语料库的单词次数查询统计字典\n",
    "\n",
    "def edits1(word):\n",
    "    alphabet = 'abcdefghijklmnopqrstuvwxyz'\n",
    "    n = len(word)\n",
    "    return set([word[0:i]+word[i+1:] for i in range(n)] +                     # deletion\n",
    "               [word[0:i]+word[i+1]+word[i]+word[i+2:] for i in range(n-1)] + # transposition\n",
    "               [word[0:i]+c+word[i+1:] for i in range(n) for c in alphabet] + # alteration\n",
    "               [word[0:i]+c+word[i:] for i in range(n+1) for c in alphabet])  # insertion\n",
    "\n",
    "def known_edits2(word):\n",
    "    return set(e2 for e1 in edits1(word) for e2 in edits1(e1) if e2 in NWORDS)\n",
    "\n",
    "def known(words): return set(w for w in words if w in NWORDS)\n",
    "\n",
    "def correct(word):\n",
    "    candidates = known([word]) or known(edits1(word)) or known_edits2(word) or [word] #可能的单词\n",
    "    return max(candidates, key=lambda w: NWORDS[w])        #从字典中查看当前单词出现次数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'words'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct('wordd')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.基于贝叶斯算法的新闻分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取数据\n",
    "import pandas as pd\n",
    "import jieba\n",
    "df_news = pd.read_table('./data/val.txt',names=['category','theme','URL','content'],encoding='utf-8')\n",
    "df_news = df_news.dropna()\n",
    "df_news.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用结巴分词器分词 将所有文档保存于content_S\n",
    "content = df_news.content.values.tolist()\n",
    "content_S = []\n",
    "for line in content:\n",
    "    current_segment = jieba.lcut(line)\n",
    "    if len(current_segment) > 1 and current_segment != '\\r\\n': #换行符\n",
    "        content_S.append(current_segment)\n",
    "\n",
    "#将文档数据转换为DateFrame格式      \n",
    "df_content=pd.DataFrame({'content_S':content_S})\n",
    "\n",
    "#读取停用词表 将文档通过停用词表过滤  去掉 词频/逆词频 值较小的词\n",
    "stopwords=pd.read_csv(\"stopwords.txt\",index_col=False,sep=\"\\t\",quoting=3,names=['stopword'], encoding='utf-8')\n",
    "\n",
    "def drop_stopwords(contents,stopwords):\n",
    "    contents_clean = []\n",
    "    all_words = []\n",
    "    for line in contents:\n",
    "        line_clean = []\n",
    "        for word in line:\n",
    "            if word in stopwords:\n",
    "                continue\n",
    "            line_clean.append(word)\n",
    "            all_words.append(str(word))\n",
    "        contents_clean.append(line_clean)\n",
    "    return contents_clean,all_words\n",
    "\n",
    "contents = df_content.content_S.values.tolist()    \n",
    "stopwords = stopwords.stopword.values.tolist()\n",
    "contents_clean,all_words = drop_stopwords(contents,stopwords)\n",
    "\n",
    "#将清理后的词转换为DataFrame格式\n",
    "df_content=pd.DataFrame({'contents_clean':contents_clean})\n",
    "\n",
    "#统计文档中所有出现的词 创建词云\n",
    "df_all_words=pd.DataFrame({'all_words':all_words})\n",
    "words_count=df_all_words.groupby(by=['all_words'])['all_words'].agg({\"count\":numpy.size})\n",
    "words_count=words_count.reset_index().sort_values(by=[\"count\"],ascending=False)\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "matplotlib.rcParams['figure.figsize'] = (10.0, 5.0)\n",
    "\n",
    "wordcloud=WordCloud(font_path=\"./data/simhei.ttf\",background_color=\"white\",max_font_size=80)\n",
    "word_frequence = {x[0]:x[1] for x in words_count.head(100).values}\n",
    "wordcloud=wordcloud.fit_words(word_frequence)\n",
    "plt.imshow(wordcloud)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  TF-IDF ：提取关键词###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora, models, similarities\n",
    "import gensim\n",
    "import jieba.analyse\n",
    "index = 2400\n",
    "print (df_news['content'][index])\n",
    "content_S_str = \"\".join(content_S[index])  \n",
    "print (\"  \".join(jieba.analyse.extract_tags(content_S_str, topK=5, withWeight=False)))\n",
    "\n",
    "# 将文档中的词进行编号 创建字典\n",
    "dictionary = corpora.Dictionary(contents_clean)\n",
    "corpus = [dictionary.doc2bow(sentence) for sentence in contents_clean]\n",
    "\n",
    "lda = gensim.models.ldamodel.LdaModel(corpus=corpus, id2word=dictionary, num_topics=20) #类似Kmeans自己指定K值 将字典传入LDA模型 完成LDA主题模型\n",
    "\n",
    "for topic in lda.print_topics(num_topics=20, num_words=5):\n",
    "    print (topic[1])\n",
    "\n",
    "#标签转换为数值\n",
    "df_train=pd.DataFrame({'contents_clean':contents_clean,'label':df_news['category']})\n",
    "print(df_train.tail())\n",
    "df_train.label.unique()\n",
    "label_mapping = {\"汽车\": 1, \"财经\": 2, \"科技\": 3, \"健康\": 4, \"体育\":5, \"教育\": 6,\"文化\": 7,\"军事\": 8,\"娱乐\": 9,\"时尚\": 0}\n",
    "df_train['label'] = df_train['label'].map(label_mapping)\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(df_train['contents_clean'].values, df_train['label'].values, random_state=1)\n",
    "\n",
    "words = []\n",
    "for line_index in range(len(x_train)):\n",
    "    try:\n",
    "        #x_train[line_index][word_index] = str(x_train[line_index][word_index])\n",
    "        words.append(' '.join(x_train[line_index]))\n",
    "    except:\n",
    "        print (line_index,word_index) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将文档内容转换成向量形式 表示每个单词出现的次数\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vec = CountVectorizer(analyzer='word', max_features=4000,  lowercase = False)\n",
    "vec.fit(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模型训练\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "classifier = MultinomialNB()\n",
    "classifier.fit(vec.transform(words), y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模型测试\n",
    "test_words = []\n",
    "for line_index in range(len(x_test)):\n",
    "    try:\n",
    "        #x_train[line_index][word_index] = str(x_train[line_index][word_index])\n",
    "        test_words.append(' '.join(x_test[line_index]))\n",
    "    except:\n",
    "         print (line_index,word_index)\n",
    "test_words[0]\n",
    "\n",
    "classifier.score(vec.transform(test_words), y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 通过TF-ID转换词模型\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(analyzer='word', max_features=4000,  lowercase = False)\n",
    "vectorizer.fit(words)\n",
    "\n",
    "#模型训练\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "classifier = MultinomialNB()\n",
    "classifier.fit(vectorizer.transform(words), y_train)\n",
    "\n",
    "#模型测试\n",
    "classifier.score(vectorizer.transform(test_words), y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
